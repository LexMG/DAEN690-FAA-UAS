{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import xml.dom.minidom\n",
    "import spacy\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define all methods necessary for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for extracting text from xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(nodelist):\n",
    "    rc = []\n",
    "    for node in nodelist:\n",
    "        if node.nodeType == node.TEXT_NODE:\n",
    "            rc.append(node.data)\n",
    "    return ''.join(rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for extracting declination at a speicific location on a specific date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDecination(latitude, longitude, date = ''):\n",
    "    # Set base URL for query\n",
    "    url = \"https://www.ngdc.noaa.gov/geomag-web/calculators/calculateDeclination?browserRequest=true&magneticComponent=d&\"\n",
    "    # Set hemisphere for lat/long\n",
    "    if latitude > 0:\n",
    "        latLabel = 'N'\n",
    "    else:\n",
    "        latLabel = 'S'\n",
    "    if longitude > 0:\n",
    "        lonLabel = 'E'\n",
    "    else:\n",
    "        lonLabel = 'W'\n",
    "    # Parse date into day, month, year\n",
    "    if date is None:\n",
    "        year = datetime.now().year\n",
    "        month = datetime.now().month\n",
    "        day = datetime.now().day\n",
    "    else:\n",
    "        year, month, day = date.split('T', 1)[0].split('-',2)\n",
    "    # Encode URL parameters\n",
    "    data = urllib.parse.urlencode({'lat1': abs(latitude), 'lat1Hemisphere': latLabel, 'lon1': abs(longitude), 'lon1Hemisphere': lonLabel, 'model': 'WMM', 'startYear': year, 'startMonth': month, 'startDay': day, 'resultFormat': 'xml'})\n",
    "    url_req = url + data\n",
    "    # Pass request to site\n",
    "    req = urllib.request.urlopen(url_req)\n",
    "    # Process XML file into object tree and get only declination info\n",
    "    dom = xml.dom.minidom.parseString(req.read().decode(\"UTF-8\"))\n",
    "    myString = getText(dom.getElementsByTagName(\"declination\")[0].childNodes)\n",
    "    # Removes formatting to leave only declination value\n",
    "    declination = str(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", myString)[0])\n",
    "    # Sets value for row\n",
    "    return declination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to convert lat/long from Degree/Minute/Second to Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dms_to_dd (loc):\n",
    "    if type(loc) == float or loc == '':\n",
    "        dd = loc\n",
    "        \n",
    "    else:\n",
    "        loc = loc.strip()\n",
    "        #loc = loc.replace('', '00-00-00.0000N')\n",
    "        d, m, s = map(float, loc[:-1].split('-'))\n",
    "        h = loc[-1]\n",
    "    \n",
    "        dd = float(d) + float(m)/60 + float(s)/(60*60);\n",
    "        if h == 'S' or h == 'W':\n",
    "            dd *= -1\n",
    "    \n",
    "    return dd;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to calculate centroid of a convex polygon from a list of vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(vertices):\n",
    "     _x_list = [vertex [0] for vertex in vertices]\n",
    "     _y_list = [vertex [1] for vertex in vertices]\n",
    "     _len = len(vertices)\n",
    "     _x = sum(_x_list) / _len\n",
    "     _y = sum(_y_list) / _len\n",
    "     return(_x, _y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and clean Incidents and all supplementary datasets for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Incidents dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv into dataframe\n",
    "incidents = pd.read_csv(\"Incident_Export_2021-08-26T08_57Z.csv\", header = 0)\n",
    "# Replace NA values with a null string\n",
    "incidents.fillna('', inplace=True)\n",
    "# Add columns for CEDAR Remarks\n",
    "incidents['EVENTTYPE.CEDAR'] = ''\n",
    "incidents['STATUS.CEDAR'] = ''\n",
    "incidents['MORID.CEDAR'] = ''\n",
    "incidents['FACILITY.CEDAR'] = ''\n",
    "incidents['EVENTDATE.CEDAR'] = ''\n",
    "incidents['UTCTIME.CEDAR'] = ''\n",
    "incidents['UTCTIME24.CEDAR'] = ''\n",
    "incidents['CALENDARDATE.CEDAR'] = ''\n",
    "incidents['NEARESTAIRPORT.CEDAR'] = ''\n",
    "incidents['METAR.CEDAR'] = ''\n",
    "incidents['POTENTIALLYSIGNIFICANT.CEDAR'] = ''\n",
    "incidents['CALLSIGN.CEDAR'] = ''\n",
    "incidents['ACTYPE.CEDAR'] = ''\n",
    "incidents['IFRIVR.CEDAR'] = ''\n",
    "incidents['AUTHCERT.CEDAR'] = ''\n",
    "incidents['AIRSPACECLASS.CEDAR'] = ''\n",
    "incidents['ACLOCATION.CEDAR'] = ''\n",
    "incidents['ACALTITUDE.CEDAR'] = ''\n",
    "incidents['ACHEADING.CEDAR'] = ''\n",
    "incidents['RELATIVECLOCKPOSITION.CEDAR'] = ''\n",
    "incidents['UASREGISTRATIONNUM.CEDAR'] = ''\n",
    "incidents['UASLONG.CEDAR'] = ''\n",
    "incidents['UASLAT.CEDAR'] = ''\n",
    "incidents['UASTYPE.CEDAR'] = ''\n",
    "incidents['UASFORMATION.CEDAR'] = ''\n",
    "incidents['CLOSESTPROXIMITY.CEDAR'] = ''\n",
    "incidents['UASWEIGHTGT55.CEDAR'] = ''\n",
    "incidents['UASDIM.CEDAR'] = ''\n",
    "incidents['UASFWROTOR.CEDAR'] = ''\n",
    "incidents['UASACTIVITYRISK.CEDAR'] = ''\n",
    "incidents['UASCOLOR.CEDAR'] = ''\n",
    "incidents['PILOTREPORTEDNMAC.CEDAR'] = ''\n",
    "incidents['TCASRA.CEDAR'] = ''\n",
    "incidents['LEOCONTACT.CEDAR'] = ''\n",
    "incidents['SUMMARY.CEDAR'] = ''\n",
    "incidents['QAFINDINGS.CEDAR'] = ''\n",
    "incidents['UAS_LATITUDE'] = ''\n",
    "incidents['UAS_LONGITUDE'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through dataset to extract each comment and assign it to \n",
    "for i in incidents.index:\n",
    "    try:\n",
    "        event_type = re.search(\"CEDAR – Event Type: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'EVENTTYPE.CEDAR'] = event_type[:-1].lstrip('CEDAR – Event Type: ')\n",
    "    except:\n",
    "        incidents.at[i, 'EVENTTYPE.CEDAR'] = ''\n",
    "    try:\n",
    "        status = re.search(\"Status: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'STATUS.CEDAR'] = status[:-1].lstrip('Status: ')\n",
    "    except:\n",
    "        incidents.at[i, 'STATUS.CEDAR'] = ''\n",
    "    try:\n",
    "        mor_id = re.search(\"MOR ID: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'MORID.CEDAR'] = mor_id[:-1].lstrip('MOR ID: ')\n",
    "    except:\n",
    "        incidents.at[i, 'MORID.CEDAR'] = ''\n",
    "    try:\n",
    "        facility = re.search(\"Facility: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'FACILITY.CEDAR'] = facility[:-1].lstrip('Facility: ')\n",
    "    except:\n",
    "        incidents.at[i, 'FACILITY.CEDAR'] = ''\n",
    "    try:\n",
    "        event_date = re.search(\"Event Date: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'EVENTDATE.CEDAR'] = event_date[:-1].lstrip('Event Date: ')\n",
    "    except:\n",
    "        incidents.at[i, 'EVENTDATE.CEDAR'] = ''\n",
    "    try:\n",
    "        utc_time = re.search(\"UTC Time: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UTCTIME.CEDAR'] = utc_time[:-1].lstrip('UTC Time: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UTCTIME.CEDAR'] = ''\n",
    "    try:\n",
    "        utc_time_24 = re.search(\"UTC Time 24 HR Format: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UTCTIME24.CEDAR'] = utc_time_24[:-1].lstrip('UTC Time 24 HR Format: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UTCTIME24.CEDAR'] = ''\n",
    "    try:\n",
    "        calendar_date = re.search(\"Calendar Date: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'CALENDARDATE.CEDAR'] = calendar_date[:-1].lstrip('Calendar Date: ')\n",
    "    except:\n",
    "        incidents.at[i, 'CALENDARDATE.CEDAR'] = ''\n",
    "    try:\n",
    "        nearest_airport = re.search(\"Nearest Airport: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'NEARESTAIRPORT.CEDAR'] = nearest_airport[:-1].lstrip('Nearest Airport: ')\n",
    "    except:\n",
    "        incidents.at[i, 'NEARESTAIRPORT.CEDAR'] = ''\n",
    "    try:\n",
    "        metar = re.search(\"METAR: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'METAR.CEDAR'] = metar[:-1].lstrip('METAR: ')\n",
    "    except:\n",
    "        incidents.at[i, 'METAR.CEDAR'] = ''\n",
    "    try:\n",
    "        pot_significant = re.search(\"Potentially Significant: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'POTENTIALLYSIGNIFICANT.CEDAR'] = pot_significant[:-1].lstrip('Potentially Significant: ')\n",
    "    except:\n",
    "        incidents.at[i, 'POTENTIALLYSIGNIFICANT.CEDAR'] = ''\n",
    "    try:\n",
    "        call_sign = re.search(\"Callsign: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'CALLSIGN.CEDAR'] = call_sign[:-1].lstrip('Callsign: ')\n",
    "    except:\n",
    "        incidents.at[i, 'CALLSIGN.CEDAR'] = ''\n",
    "    try:\n",
    "        ac_type = re.search(\"A/C Type: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'ACTYPE.CEDAR'] = ac_type[:-1].lstrip('A/C Type: ')\n",
    "    except:\n",
    "        incidents.at[i, 'ACTYPE.CEDAR'] = ''\n",
    "    try:\n",
    "        ifr_ivr = re.search(\"IFR / IVR: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'IFRIVR.CEDAR'] = ifr_ivr[:-1].lstrip('IFR / IVR: ')\n",
    "    except:\n",
    "        incidents.at[i, 'IFRIVR.CEDAR'] = ''\n",
    "    try:\n",
    "        auth_cert = re.search(\"Certificate of Authorization: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'AUTHCERT.CEDAR'] = auth_cert[:-1].lstrip('Certificate of Authorization: ')\n",
    "    except:\n",
    "        incidents.at[i, 'AUTHCERT.CEDAR'] = ''\n",
    "    try:\n",
    "        airspace_class = re.search(\"Airspace Class: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'AIRSPACECLASS.CEDAR'] = airspace_class[:-1].lstrip('Airspace Class: ')\n",
    "    except:\n",
    "        incidents.at[i, 'AIRSPACECLASS.CEDAR'] = ''\n",
    "    try:\n",
    "        ac_location = re.search(\"A/C Location F/R/D: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'ACLOCATION.CEDAR'] = ac_location[:-1].lstrip('A/C Location: ')\n",
    "    except:\n",
    "        incidents.at[i, 'ACLOCATION.CEDAR'] = ''\n",
    "    try:\n",
    "        ac_altitude = re.search(\"A/C Altitude: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'ACALTITUDE.CEDAR'] = ac_altitude[:-1].lstrip('A/C Altitude: ')\n",
    "    except:\n",
    "        incidents.at[i, 'ACALTITUDE.CEDAR'] = ''\n",
    "    try:\n",
    "        ac_heading = re.search(\"A/C Heading: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'ACHEADING.CEDAR'] = ac_heading[:-1].lstrip('A/C Heading: ')\n",
    "    except:\n",
    "        incidents.at[i, 'ACHEADING.CEDAR'] = ''\n",
    "    try:\n",
    "        rcp = re.search(\"Relative Clock Position: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'RELATIVECLOCKPOSITION.CEDAR'] = rcp[:-1].lstrip('Relative Clock Position: ')\n",
    "    except:\n",
    "        incidents.at[i, 'RELATIVECLOCKPOSITION.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_reg_num = re.search(\"UAS Registration #: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASREGISTRATIONNUM.CEDAR'] = uas_reg_num[:-1].lstrip('UAS Registration #: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASREGISTRATIONNUM.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_long = re.search(\"UAS Longitude: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASLONG.CEDAR'] = uas_long[:-1].lstrip('UAS Longitude: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASLONG.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_lat = re.search(\"UAS Latitude: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASLAT.CEDAR'] = uas_lat[:-1].lstrip('UAS Latitude: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASLAT.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_type = re.search(\"UAS Type: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASTYPE.CEDAR'] = uas_type[:-1].lstrip('UAS Type: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASTYPE.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_formation = re.search(\"UAS Formation: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASFORMATION.CEDAR'] = uas_formation[:-1].lstrip('UAS Formation: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASFORMATION.CEDAR'] = ''\n",
    "    try:\n",
    "        closest_prox = re.search(\"Closest Proximity (feet): (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'CLOSESTPROXIMITY.CEDAR'] = closest_prox[:-1].lstrip('Closest Proximity (feet): ')\n",
    "    except:\n",
    "        incidents.at[i, 'CLOSESTPROXIMITY.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_weight = re.search(\"UAS Weight Exceeds 55lbs: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASWEIGHTGT55.CEDAR'] = uas_weight[:-1].lstrip('UAS Weight Exceeds 55lbs: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASWEIGHTGT55.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_dim = re.search(\"UAS Dimensions (feet): (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASDIM.CEDAR'] = uas_dim[:-1].lstrip('UAS Dimensions (feet): ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASDIM.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_fwr = re.search(\"UAS Fixed Wing/Rotorcraft: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASFWROTOR.CEDAR'] = uas_fwr[:-1].lstrip('UAS Fixed Wing/Rotorcraft: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASFWROTOR.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_act_risk = re.search(\"UAS Activity Risk: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASACTIVITYRISK.CEDAR'] = uas_act_risk[:-1].lstrip('UAS Activity Risk: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASACTIVITYRISK.CEDAR'] = ''\n",
    "    try:\n",
    "        uas_color = re.search(\"UAS Color: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'UASCOLOR.CEDAR'] = uas_color[:-1].lstrip('UAS Color: ')\n",
    "    except:\n",
    "        incidents.at[i, 'UASCOLOR.CEDAR'] = ''\n",
    "    try:\n",
    "        pilot_rep_dnmac = re.search(\"Pilot Reported as NMAC: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'PILOTREPORTEDNMAC.CEDAR'] = pilot_rep_dnmac[:-1].lstrip('Pilot Reported as NMAC: ')\n",
    "    except:\n",
    "        incidents.at[i, 'PILOTREPORTEDNMAC.CEDAR'] = ''\n",
    "    try:\n",
    "        tcas_ra = re.search(\"TCAS RA: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'TCASRA.CEDAR'] = tcas_ra[:-1].lstrip('TCAS RA: ')\n",
    "    except:\n",
    "        incidents.at[i, 'TCASRA.CEDAR'] = ''\n",
    "    try:\n",
    "        leo_contact = re.search(\"Law Enforcement Contact Info: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'LEOCONTACT.CEDAR'] = leo_contact[:-1].lstrip('Law Enforcement Contact Info: ')\n",
    "    except:\n",
    "        incidents.at[i, 'LEOCONTACT.CEDAR'] = ''\n",
    "    try:\n",
    "        summary = re.search(\"Summary: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'SUMMARY.CEDAR'] = summary[:-1].lstrip('Summary: ')\n",
    "    except:\n",
    "        incidents.at[i, 'SUMMARY.CEDAR'] = ''\n",
    "    try:\n",
    "        qa_findings = re.search(\"QA Findings: (.*?);\", incidents['CEDAR.REMARKS'][i]).group()\n",
    "        incidents.at[i, 'QAFINDINGS.CEDAR'] = qa_findings[:-1].lstrip('QA Findings: ')\n",
    "    except:\n",
    "        incidents.at[i, 'QAFINDINGS.CEDAR'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needs reviewed to convert from R to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize references to runways\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('(Runway|runway|RUNWAY|RY)','RWY',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('(Runway|runway|RUNWAY|RY)','RWY',regex=True)\n",
    "# Standardize 'of the' to 'of'\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('of the','of',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('of the','of',regex=True)\n",
    "# Standardize nautical/miles to NM\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('(M|m)iles?','NM',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('(M|m)iles?','NM',regex=True)\n",
    "# Standardize UAS to lowercase to ease parsing alphabetical codes later\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('UAS','uas',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('UAS','uas',regex=True)\n",
    "# I have no clue what this does\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('(\\\\+\\\\d{1,2}\\\\s?)?1?\\\\-?\\\\.?\\\\s?\\\\(?\\\\d{3}\\\\)?[\\\\s.-]?\\\\d{3}[\\\\s.-]?\\\\d{4}','',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('(\\\\+\\\\d{1,2}\\\\s?)?1?\\\\-?\\\\.?\\\\s?\\\\(?\\\\d{3}\\\\)?[\\\\s.-]?\\\\d{3}[\\\\s.-]?\\\\d{4}','',regex=True)\n",
    "# I'm sure this does important things as well\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('(NM)([A-Z]{1,3})','\\\\1 \\\\2',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('(NM)([A-Z]{1,3})','\\\\1 \\\\2',regex=True)\n",
    "# Not even going to pretend I know\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('([0-9]*\\\\-*\\\\/*[0-9]*\\\\.*[0-9]\\\\.*[0-9]*)(\\\\NM)','\\\\1 \\\\2',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('([0-9]*\\\\-*\\\\/*[0-9]*\\\\.*[0-9]\\\\.*[0-9]*)(\\\\NM)','\\\\1 \\\\2',regex=True)\n",
    "# I should probably look all this stuff up\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('(of)([A-Z]{3,4})','\\\\1 \\\\2',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('(of)([A-Z]{3,4})','\\\\1 \\\\2',regex=True)\n",
    "# I'm also copy/pasting from R code\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('([A-Z]{1,3})(of)','\\\\1 \\\\2',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('([A-Z]{1,3})(of)','\\\\1 \\\\2',regex=True)\n",
    "# I sure do hope the syntax is the same\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('([A-Z]{1,3})(\\\\s[A-Z]{3,4}$)','\\\\1 of\\\\2',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('([A-Z]{1,3})(\\\\s[A-Z]{3,4}$)','\\\\1 of\\\\2',regex=True)\n",
    "# It would really suck to have to go back and fix this\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('(RWY)([0-9]{1,2}[L|R|C]?)','\\\\1 of\\\\2',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('(RWY)([0-9]{1,2}[L|R|C]?)','\\\\1 of\\\\2',regex=True)\n",
    "# Standardize UAS to lowercase to ease parsing alphabetical codes later\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('(RWY\\\\s)(\\\\d(?!\\\\d))','\\\\10\\\\2',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('(RWY\\\\s)(\\\\d(?!\\\\d))','\\\\10\\\\2',regex=True)\n",
    "# Standardize double spaces to single spaces\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('  ',' ',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('  ',' ',regex=True)\n",
    "# Standardize S/south to S\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('south|South','S',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('south|South','S',regex=True)\n",
    "# Standardize E/east to E\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('east|East','E',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('east|East','E',regex=True)\n",
    "# Standardize N/north to N\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('north|North','N',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('north|North','N',regex=True)\n",
    "# Standardize W/west to W\n",
    "incidents['REMARKS'] = incidents['REMARKS'].str.replace('west|West','W',regex=True)\n",
    "incidents['SUMMARY.CEDAR'] = incidents['SUMMARY.CEDAR'].str.replace('west|West','W',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Airports datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set URLs for API call\n",
    "airport_url = \"https://opendata.arcgis.com/api/v3/datasets/e747ab91a11045e8b3f8a3efd093d3b5_0/downloads/data?format=csv&spatialRefId=4326\"\n",
    "# Pull CSV from FAA site to dataframe\n",
    "airport = pd.read_csv(airport_url)\n",
    "# Remove unnecessary rows and rename for readability\n",
    "airport_df = airport[['GLOBAL_ID', 'IDENT', 'ICAO_ID', 'NAME', 'LONGITUDE', 'LATITUDE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract NAVAIDs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set URLs for API call\n",
    "navaid_url = \"https://opendata.arcgis.com/api/v3/datasets/990e238991b44dd08af27d7b43e70b92_0/downloads/data?format=csv&spatialRefId=4326\"\n",
    "# Pull CSV from FAA site to dataframe\n",
    "navaid = pd.read_csv(navaid_url)\n",
    "# Remove unnecessary rows and rename for readability\n",
    "navaid_df = navaid[['GLOBAL_ID', 'IDENT', 'NAME_TXT', 'X', 'Y', ]]\n",
    "navaid_df = navaid_df.rename(columns={\"X\": \"LONGITUDE\", \"Y\": \"LATITUDE\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Runways dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass request to site for GeoJSON\n",
    "runway_url = \"https://opendata.arcgis.com/datasets/4d8fa46181aa470d809776c57a8ab1f6_0.geojson\"\n",
    "req = urllib.request.urlopen(runway_url)\n",
    "# Read into JSON\n",
    "data = req.read()\n",
    "runway_json = json.loads(data.decode('utf-8'))\n",
    "# Read JSON into dataframe\n",
    "runway_df = pd.json_normalize(runway_json, record_path =['features'])\n",
    "# Add lat/long for runway center\n",
    "runway_df['RUNWAY.LATITUDE'] = ''\n",
    "runway_df['RUNWAY.LONGITUDE'] = ''\n",
    "# Extract list of coordinates from nested list\n",
    "for i in runway_df.index:\n",
    "    runway_df.at[i, 'geometry.coordinates'] = runway_df['geometry.coordinates'][i][0]\n",
    "# Iterate through dataframe to calculate center of runway and split into lat/long values\n",
    "for i in runway_df.index:\n",
    "    coords = centroid(runway_df['geometry.coordinates'][i])\n",
    "    runway_df.at[i, 'RUNWAY.LATITUDE'] = coords[1]\n",
    "    runway_df.at[i, 'RUNWAY.LONGITUDE'] = coords[0]\n",
    "# Remove unnecessary rows and rename for readability\n",
    "runway_df = runway_df[['properties.GLOBAL_ID','properties.AIRPORT_ID','properties.DESIGNATOR','RUNWAY.LATITUDE','RUNWAY.LONGITUDE']]\n",
    "runway_df = runway_df.rename({'properties.GLOBAL_ID': 'GLOBAL_ID','properties.AIRPORT_ID': 'AIRPORT_ID','properties.DESIGNATOR': 'DESIGNATOR'}, axis=1)\n",
    "# Inner join of Runways and Airports on Runways.AIRPORT_ID and Airports.GLOBAL_ID\n",
    "runway_df = pd.merge(runway_df, airport_df, how='inner', left_on = 'AIRPORT_ID', right_on = 'GLOBAL_ID')\n",
    "# Remove unnecessary rows for readability\n",
    "runway_df = runway_df[['DESIGNATOR','RUNWAY.LATITUDE','RUNWAY.LONGITUDE','IDENT','ICAO_ID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Designated Points dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set URL for API call\n",
    "point_url = \"https://opendata.arcgis.com/datasets/861043a88ff4486c97c3789e7dcdccc6_0.geojson\"\n",
    "# Pass request to site for GeoJSON\n",
    "point_req = urllib.request.urlopen(point_url)\n",
    "# Read into JSON\n",
    "point_data = point_req.read()\n",
    "point_json = json.loads(point_data.decode('utf-8'))\n",
    "# Read JSON into dataframe\n",
    "point_df = pd.json_normalize(point_json, record_path =['features'])\n",
    "# Add lat/long for runway center\n",
    "point_df['LATITUDE'] = ''\n",
    "point_df['LONGITUDE'] = ''\n",
    "# Iterate through to calculate latitude/longitude for each point\n",
    "# Needs if/then check to account for missing values in columns\n",
    "for i in point_df.index:\n",
    "    # Checks to see if lat/long is empty and converts from dms to dd if not\n",
    "    if point_df['properties.LATITUDE'][i] is not None:\n",
    "        point_df.at[i, 'LATITUDE'] =  dms_to_dd(point_df.at[i, 'properties.LATITUDE'])\n",
    "        point_df.at[i, 'LONGITUDE'] = dms_to_dd(point_df.at[i, 'properties.LONGITUDE'])\n",
    "    # Uses coordinates for remaining points\n",
    "    else:\n",
    "        point_df.at[i, 'LATITUDE'] =  point_df.at[i, 'geometry.coordinates'][1]\n",
    "        point_df.at[i, 'LONGITUDE'] =  point_df.at[i, 'geometry.coordinates'][0]\n",
    "# Remove unnecessary rows and rename for readability\n",
    "point_df = point_df[['properties.GLOBAL_ID', 'properties.IDENT','LATITUDE','LONGITUDE']]\n",
    "point_df = point_df.rename({'properties.GLOBAL_ID': 'GLOBAL_ID', 'properties.IDENT': 'IDENT'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate UAS locations from Incidents dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run R scripts from Lex here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to .shp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set geometry type\n",
    "geometry = gpd.points_from_xy([0], [0])\n",
    "# Convert incidents to GeoDataFrame\n",
    "incidents = gpd.GeoDataFrame(geometry = geometry)\n",
    "# Export shapefile into zip folder\n",
    "incidents.to_file('UAS_Incidents_Final.zip', driver = 'ESRI Shapefile')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
